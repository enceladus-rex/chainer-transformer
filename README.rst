Attention Is All You Need Paper Implementation
==============================================

This is an unaffiliated implementation of the following paper:

    Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, ≈Å. and Polosukhin, I., 2017. Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).


.. raw:: html
    
    <h2>Motivation</h2>

Prior to the wave of transformer-style attention in the NLP and wider deep learning communities, language models 
were moving much more towards applying recurrent networks to tackle complex linguistic tasks.
In particular, for the class of problems that take an input sequence and transform it into an output
sequence (known as sequence transduction), recurrent networks were the favored approach. 
The main motivation behind the paper was the observation that although recurrent networks 
as they were formulated could capture long-term relationships, there was usually a hard limit 
on how far back information would propagate due to the vanishing gradient problem. Another limiting factor
when working with long sequences is that recurrent networks pass information sequentially which can be
inefficient.

The paper introduced the Transformer model, which uses a special kind of global attention that
is able to transform an input sequence of length N into an output sequence of length M, using O(M)
forward passes, where each forward pass can attend to the inputs and previously produced outputs in parallel.
By structuring learning in this way, some of the aforementioned problems that faced
recurrent models could be greatly diminished.

.. raw:: html
    
    <h2>Key Contributions</h2>

* First neural sequence transduction model that worked entirely on self-attention.
* Outperformed all previous models on the WMT 2014 English-to-German and English-to-French tasks.

.. raw:: html
    
    <h2>Approach</h2>

As described, the main goal of the Transformer is to take an input sequence of tokens (i.e. words or subword units)
and transform them into an output sequence of tokens. A sequence of tokens, however, can be represented
in any number of different ways and so the first step in this process requires that a
suitable representation be chosen. For the Transformer model, it is expected
that the input token sequence (i.e. a sentence) is converted into a sequence of vectors known as embeddings.
Such embeddings have been shown to be powerful ways to capture semantic meaning
in a vector space. This is typically done by running a separate training process, using
methods such as Word2Vec or GloVe, to learn a representation in which tokens that
co-occur are closer in the vector space. There are many pre-trained token (i.e. word) embeddings
that can be used off-the-shelf, though they can also be generated using existing tools relatively
easily as well. In sum, this pre-processing step takes an input sequence of tokens
(which are often words but can also be subword units) and uses what is effectively a learned
lookup table to convert that into a sequence of vectors of a common dimensionality (i.e. 300).
This can be represented as a 2D matrix of shape [number of input tokens, input embedding dimensionality].
In the paper this sequence of vectors is described as the ``Input Embeddings``.

Although the Transformer does not maintain a hidden state which gets updated sequentially
like an RNN, it still generates the sequence of output tokens autoregressively one at a time.
What this means is that to generate M output tokens requires running M different forward
passes. Bare in mind, that while the actual output generated by the transformer at each of these
M steps is a discrete categorical distribution over the fixed number of possible output tokens, this
can also be converted into a token embedding in much the same way as described for the inputs. 
Specifically, the most likely output token index (highest probability) is directly looked up in a 
learned output embedding table just as before. At each of the M steps, one can then represent the
previously generated output tokens using a 2D matrix of shape 
[number of output tokens so far, output embedding dimensionality]. This matrix
is described in the paper as ``Output Embeddings``. Ultimately, this means that both the 
input tokens and output tokens have semantic vector representations (embeddings) which can be 
directly leveraged for neural processing.

There is one important consideration that should also be made when using token embeddings.
When training sequence transducers, there are often a set of special tokens that are interpreted
as signals to either begin or end a sequence. Specifically in the case of the Transformer, before  
producing any outputs, the output embeddings are initialized using a special "begin" embedding. 
Subsequent predicted embeddings are then concatenated with this "begin" embedding to produce the 
output embeddings for future steps. Additionally, the Transformer signals that it is finished with
sequence transduction by predicting the special "end" token as the most likely in its output 
distribution.

The Transformer's neural architecture can be decomposed into two components, the encoder and
the decoder. The encoder is a neural module that takes as input the previously described input 
embeddings with a separate positional embedding added to it. A positional embedding is just
another vector of the same dimensionality as the token embedding it is used with, but that
contains information specific to the position in the input sequence it is associated with.
The paper describes the exact positional embedding used, but note that it is not a 
learned embedding and is just computed using a function of the position in the sequence and 
the size of the embedding space. The encoder produces a sequence of encodings with the same
length as the input. For every input sequence, the encoder is run only a single time.
The decoder, on the other hand is, a separate neural module that unlike the encoder is run M 
times, once for each output token. As described earlier, the output of the decoder is a discrete 
categorical distribution over the set of possible output tokens. Additionally, there are two inputs 
to the decoder, the encodings (from the encoder) and the aforementioned output embeddings with a 
positional embedding added to them just like with the encoder inputs.

Both the encoder and the decoder heavily use what is known as multi-head scaled dot-product attention (MHSDPA).
This attention mechanism is used in cases where given three sequences of vectors, Q, K, and V of
lengths L, J, and J respectively, you want to generate an output sequence of vectors Z of length L in such a way
that each output vector is produced by attending over all of the J elements in sequence K for each
of the L elements in sequence Q. Additionally, the output vector should be produced mainly by "looking" at 
the vectors in the sequence V. The matrices Q, K, and V and dubbed queries, keys, and values respectively
because this mechanism can be described as using using each query to lookup values using the keys and finally
computing an output. Dot product attention accomplishes this by first linearly mapping the 
queries (Q) of length L and keys (K) of length J in vectors with the same dimensionality.
Additionally, the J values in V are linearly mapped to vectors with the desired dimensionality of the output.
Each of the J output vectors of the sequence Z are then computed using a convex 
combination of these mapped values. The weights of the convex combination for the output at position
i are generated by first taking the dot-product of the linearly mapped query at position i with all of the
linearly mapped keys. This produces J weights, one corresponding to each of the keys. These weights are then
scaled using a constant that depends on the dimensionality used for the queries and keys in order to avoid 
oversaturation of the softmax applied right afterwards to produce convex combination weights. 
Then as mentioned, the final output
is computed by taking a combination of the J linearly mapped values using these weights.
This process is done for each of the L queries producing L outputs. All of this can be accomplished
efficiently using solely matrix operations, which are detailed in in the paper. Furthermore,
by applying this multiple times using different parameters and stacking the outputs, you arrive
at the final multi-headed scaled dot-product attention (MHSDPA) mechanism.

MHSDPA is used in the encoder for self-attention. Specifically, all three input sequences, Q, K, and V,
used earlier are the same in this case and correspond to the encoder inputs. The decoder
applies MHSDPA in two ways, both for self-attention on the summed output and positional embeddings
as well as for attention using the encodings as the queries and keys while the decoder's
self-attention as the values. On important detail was that the self-attention used by the
decoder applied masking of the dot-product weights so that the convex combination for output
as position i would never include information at later time steps. This enforces the autoregressive
nature of the model.

Along with MHSDPA, feed forward networks were used as well as layer normalization and residual connections.
These operations were combined into stacks which were repeated multiple times to produce deep
models. As mentioned, the decoder also had a final linear layer with a softmax applied to generate the
predicted token probabilities for each step.

See the paper for more details.